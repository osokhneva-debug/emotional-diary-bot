# analysis.py
import json
import csv
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Tuple, Optional
from collections import Counter, defaultdict
import io
import pytz

from db import get_session, User, Entry
from i18n import TEXTS, EMOTION_CATEGORIES

class EmotionAnalyzer:
    """Analyzes emotional data and generates insights"""
    
    def __init__(self):
        # Emotion grouping based on scientific research
        self.emotion_groups = {
            'recovery_growth': {
                'name': 'üå± –≠–º–æ—Ü–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞',
                'categories': ['–†–∞–¥–æ—Å—Ç—å/–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '–ò–Ω—Ç–µ—Ä–µ—Å/–õ—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ', '–°–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ/–£–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '–≠–Ω–µ—Ä–≥–∏—á–Ω–æ—Å—Ç—å/–í–æ–æ–¥—É—à–µ–≤–ª–µ–Ω–∏–µ'],
                'description': '–≠–º–æ—Ü–∏–∏, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—â–∏–µ —Ä–∞–∑–≤–∏—Ç–∏—é –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é'
            },
            'tension_signal': {
                'name': 'üå™ –≠–º–æ—Ü–∏–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Å–∏–≥–Ω–∞–ª–∞',
                'categories': ['–¢—Ä–µ–≤–æ–≥–∞/–ë–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '–ì—Ä—É—Å—Ç—å/–ü–µ—á–∞–ª—å', '–ó–ª–æ—Å—Ç—å/–†–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ', '–°—Ç—ã–¥/–í–∏–Ω–∞', '–£—Å—Ç–∞–ª–æ—Å—Ç—å/–ò—Å—Ç–æ—â–µ–Ω–∏–µ', '–£–¥–∏–≤–ª–µ–Ω–∏–µ/–®–æ–∫'],
                'description': '–≠–º–æ—Ü–∏–∏, —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–µ –æ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è—Ö –∏ –≤—ã–∑–æ–≤–∞—Ö'
            },
            'neutral': {
                'name': '‚öñ –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è',
                'categories': [],
                'description': '–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è'
            }
        }
    
    async def generate_summary(self, user_id: int, period: str) -> str:
        """Generate emotional summary for specified period"""
        try:
            # Parse period
            days = self._parse_period(period)
            if not days:
                return TEXTS['invalid_period']
            
            # Get entries for the period
            entries = self._get_entries_for_period(user_id, days)
            
            if not entries:
                return TEXTS['no_data_for_period'].format(period=days)
            
            # Generate comprehensive analysis
            summary_parts = []
            
            # Header
            summary_parts.append(f"üìä <b>–ê–Ω–∞–ª–∏–∑ –∑–∞ {days} –¥–Ω–µ–π</b>")
            summary_parts.append(f"üìÖ –ó–∞–ø–∏—Å–µ–π: {len(entries)}")
            summary_parts.append("")
            
            # Emotion groups analysis
            group_analysis = self._analyze_emotion_groups(entries)
            summary_parts.append("üé≠ <b>–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø:</b>")
            
            for group_key, group_data in self.emotion_groups.items():
                count = group_analysis.get(group_key, 0)
                percentage = (count / len(entries) * 100) if entries else 0
                summary_parts.append(f"{group_data['name']}: {count} ({percentage:.1f}%)")
            
            summary_parts.append("")
            
            # Most frequent emotions
            emotion_freq = self._get_emotion_frequency(entries)
            if emotion_freq:
                summary_parts.append("üèÜ <b>–ß–∞—Å—Ç—ã–µ —ç–º–æ—Ü–∏–∏:</b>")
                for emotion, count in emotion_freq.most_common(5):
                    summary_parts.append(f"‚Ä¢ {emotion}: {count} —Ä–∞–∑")
                summary_parts.append("")
            
            # Trigger analysis
            trigger_analysis = self._analyze_triggers(entries)
            if trigger_analysis:
                summary_parts.append("üéØ <b>–û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã:</b>")
                for trigger, emotions in list(trigger_analysis.items())[:3]:
                    emotions_str = ", ".join(emotions[:3])
                    summary_parts.append(f"‚Ä¢ {trigger}: {emotions_str}")
                summary_parts.append("")
            
            # Time patterns
            time_patterns = self._analyze_time_patterns(entries)
            if time_patterns:
                summary_parts.append("‚è∞ <b>–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:</b>")
                peak_hour = max(time_patterns.items(), key=lambda x: x[1])
                summary_parts.append(f"‚Ä¢ –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {peak_hour[0]}:00 ({peak_hour[1]} –∑–∞–ø–∏—Å–µ–π)")
                summary_parts.append("")
            
            # Personalized insights for working women
            insights = self._generate_personalized_insights(entries, group_analysis)
            if insights:
                summary_parts.append("üí° <b>–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã:</b>")
                for insight in insights:
                    summary_parts.append(f"‚Ä¢ {insight}")
                summary_parts.append("")
            
            # Valence and arousal trends
            valence_avg = sum(e.valence for e in entries) / len(entries)
            arousal_avg = sum(e.arousal for e in entries) / len(entries)
            
            summary_parts.append("üìà <b>–û–±—â–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏:</b>")
            summary_parts.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å: {self._format_valence(valence_avg)}")
            summary_parts.append(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è: {self._format_arousal(arousal_avg)}")
            
            # Footer with scientific note
            summary_parts.append("")
            summary_parts.append("üî¨ <i>–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –ø–æ–º–æ–≥–∞–µ—Ç —Ä–∞–∑–≤–∏–≤–∞—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å –∏ —É–ª—É—á—à–∞—Ç—å —Å–∞–º–æ—Ä–µ–≥—É–ª—è—Ü–∏—é.</i>")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: {str(e)}"
    
    def _parse_period(self, period: str) -> Optional[int]:
        """Parse period string to number of days"""
        period_map = {
            "7": 7,
            "14": 14,
            "30": 30,
            "90": 90
        }
        return period_map.get(period)
    
    def _get_entries_for_period(self, user_id: int, days: int) -> List[Entry]:
        """Get entries for the specified period"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        
        with get_session() as session:
            return session.query(Entry).filter(
                Entry.user_id == user_id,
                Entry.timestamp >= cutoff_date
            ).order_by(Entry.timestamp.desc()).all()
    
    def _analyze_emotion_groups(self, entries: List[Entry]) -> Dict[str, int]:
        """Analyze emotion distribution by groups"""
        group_counts = defaultdict(int)
        
        for entry in entries:
            category = entry.category
            
            # Find which group this category belongs to
            group_found = False
            for group_key, group_data in self.emotion_groups.items():
                if category in group_data['categories']:
                    group_counts[group_key] += 1
                    group_found = True
                    break
            
            # If not found in specific groups, count as neutral
            if not group_found:
                group_counts['neutral'] += 1
        
        return dict(group_counts)
    
    def _get_emotion_frequency(self, entries: List[Entry]) -> Counter:
        """Get frequency of specific emotions"""
        emotion_counts = Counter()
        
        for entry in entries:
            if entry.emotions:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                for emotion in emotions:
                    emotion_counts[emotion] += 1
        
        return emotion_counts
    
    def _analyze_triggers(self, entries: List[Entry]) -> Dict[str, List[str]]:
        """Analyze what triggers different emotions"""
        trigger_emotions = defaultdict(set)
        
        for entry in entries:
            if entry.cause and entry.emotions:
                cause = entry.cause.strip()
                if len(cause) > 3:  # Filter out very short causes
                    emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                    for emotion in emotions:
                        trigger_emotions[cause].add(emotion)
        
        # Convert sets to lists and filter by frequency
        result = {}
        for trigger, emotions in trigger_emotions.items():
            if len(emotions) >= 1:  # At least 1 emotion
                result[trigger] = list(emotions)
        
        return result
    
    def _analyze_time_patterns(self, entries: List[Entry]) -> Dict[int, int]:
        """Analyze emotional activity by hour of day"""
        hour_counts = defaultdict(int)
        
        for entry in entries:
            hour = entry.timestamp.hour
            hour_counts[hour] += 1
        
        return dict(hour_counts)
    
    def _generate_personalized_insights(self, entries: List[Entry], group_analysis: Dict[str, int]) -> List[str]:
        """Generate personalized insights for working women"""
        insights = []
        total_entries = len(entries)
        
        if total_entries == 0:
            return insights
        
        # Analyze work-life balance patterns
        work_related_emotions = self._count_work_related_emotions(entries)
        evening_stress = self._count_evening_stress(entries)
        morning_anxiety = self._count_morning_anxiety(entries)
        
        # Insight about morning anxiety
        if morning_anxiety > total_entries * 0.3:
            insights.append("–ó–∞–º–µ—á–µ–Ω–∞ —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å –ø–æ —É—Ç—Ä–∞–º - –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—ã—Ö–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –¥–Ω—è")
        
        # Insight about evening stress
        if evening_stress > total_entries * 0.25:
            insights.append("–í–µ—á–µ—Ä–Ω–∏–π —Å—Ç—Ä–µ—Å—Å –º–æ–∂–µ—Ç –≥–æ–≤–æ—Ä–∏—Ç—å –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ª—É—á—à–∏—Ö –≥—Ä–∞–Ω–∏—Ü –º–µ–∂–¥—É —Ä–∞–±–æ—Ç–æ–π –∏ –ª–∏—á–Ω–æ–π –∂–∏–∑–Ω—å—é")
        
        # Insight about emotion balance
        tension_ratio = group_analysis.get('tension_signal', 0) / total_entries
        if tension_ratio > 0.7:
            insights.append("–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π - –≤–∞–∂–Ω–æ —É–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é")
        elif tension_ratio < 0.3:
            insights.append("–•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —ç–º–æ—Ü–∏–π - –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –ø—Ä–∞–∫—Ç–∏–∫–∏ —Å–∞–º–æ–∑–∞–±–æ—Ç—ã")
        
        # Insight about emotional variety
        unique_emotions = len(self._get_unique_emotions(entries))
        if unique_emotions < 5:
            insights.append("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å —Å–≤–æ–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è")
        
        # Work-related stress insights
        if work_related_emotions > total_entries * 0.4:
            insights.append("–ú–Ω–æ–≥–æ —Ä–∞–±–æ—á–∏—Ö –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–π - –ø–æ–¥—É–º–∞–π—Ç–µ –æ —Ç–µ—Ö–Ω–∏–∫–∞—Ö –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ä–∞–±–æ—Ç—ã")
        
        return insights[:3]  # Return max 3 insights
    
    def _count_work_related_emotions(self, entries: List[Entry]) -> int:
        """Count emotions related to work"""
        work_keywords = ['—Ä–∞–±–æ—Ç', '–∫–æ–ª–ª–µ–≥', '–±–æ—Å—Å', '–ø—Ä–æ–µ–∫—Ç', '–¥–µ–¥–ª–∞–π–Ω', '—Å–æ–≤–µ—â', '–æ—Ñ–∏—Å', '–∫–ª–∏–µ–Ω—Ç']
        count = 0
        
        for entry in entries:
            if entry.cause:
                cause_lower = entry.cause.lower()
                if any(keyword in cause_lower for keyword in work_keywords):
                    count += 1
        
        return count
    
    def _count_evening_stress(self, entries: List[Entry]) -> int:
        """Count stressful emotions in evening hours (18-23)"""
        stress_emotions = ['—Ç—Ä–µ–≤–æ–≥–∞', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '—É—Å—Ç–∞–ª–æ—Å—Ç—å', '–∏—Å—Ç–æ—â–µ–Ω–∏–µ', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ', '—Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è']
        count = 0
        
        for entry in entries:
            if 18 <= entry.timestamp.hour <= 23:
                if entry.emotions:
                    emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                    if any(emotion.lower() in stress_emotions for emotion in emotions):
                        count += 1
        
        return count
    
    def _count_morning_anxiety(self, entries: List[Entry]) -> int:
        """Count anxiety-related emotions in morning hours (6-11)"""
        anxiety_emotions = ['—Ç—Ä–µ–≤–æ–≥–∞', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ', '—Å–æ–º–Ω–µ–Ω–∏–µ']
        count = 0
        
        for entry in entries:
            if 6 <= entry.timestamp.hour <= 11:
                if entry.emotions:
                    emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                    if any(emotion.lower() in anxiety_emotions for emotion in emotions):
                        count += 1
        
        return count
    
    def _get_unique_emotions(self, entries: List[Entry]) -> set:
        """Get set of unique emotions from entries"""
        unique_emotions = set()
        
        for entry in entries:
            if entry.emotions:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                unique_emotions.update(emotions)
        
        return unique_emotions
    
    def _format_valence(self, valence: float) -> str:
        """Format valence value to human-readable string"""
        if valence > 0.3:
            return "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è üòä"
        elif valence < -0.3:
            return "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è üòî"
        else:
            return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è üòê"
    
    def _format_arousal(self, arousal: float) -> str:
        """Format arousal value to human-readable string"""
        if arousal > 1.3:
            return "–í—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è ‚ö°"
        elif arousal > 0.7:
            return "–°—Ä–µ–¥–Ω—è—è —ç–Ω–µ—Ä–≥–∏—è üåä"
        else:
            return "–ù–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è üò¥"
    
    def generate_csv_export(self, entries: List[Entry]) -> io.BytesIO:
        """Generate CSV export of emotional data"""
        output = io.StringIO()
        
        fieldnames = [
            '–î–∞—Ç–∞', '–í—Ä–µ–º—è', '–í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å', '–ê–∫—Ç–∏–≤–∞—Ü–∏—è', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è',
            '–≠–º–æ—Ü–∏–∏', '–ü—Ä–∏—á–∏–Ω–∞', '–¢–µ–ª–µ—Å–Ω—ã–µ_–æ—â—É—â–µ–Ω–∏—è', '–ó–∞–º–µ—Ç–∫–∏', '–¢–µ–≥–∏'
        ]
        
        writer = csv.DictWriter(output, fieldnames=fieldnames)
        writer.writeheader()
        
        for entry in entries:
            emotions_str = ', '.join(json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions) if entry.emotions else ''
            tags_str = ', '.join(json.loads(entry.tags) if isinstance(entry.tags, str) else entry.tags) if entry.tags else ''
            
            writer.writerow({
                '–î–∞—Ç–∞': entry.timestamp.strftime('%Y-%m-%d'),
                '–í—Ä–µ–º—è': entry.timestamp.strftime('%H:%M:%S'),
                '–í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å': entry.valence,
                '–ê–∫—Ç–∏–≤–∞—Ü–∏—è': entry.arousal,
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': entry.category or '',
                '–≠–º–æ—Ü–∏–∏': emotions_str,
                '–ü—Ä–∏—á–∏–Ω–∞': entry.cause or '',
                '–¢–µ–ª–µ—Å–Ω—ã–µ_–æ—â—É—â–µ–Ω–∏—è': entry.body_sensations or '',
                '–ó–∞–º–µ—Ç–∫–∏': entry.notes or '',
                '–¢–µ–≥–∏': tags_str
            })
        
        # Convert to bytes
        output.seek(0)
        csv_bytes = io.BytesIO()
        csv_bytes.write(output.getvalue().encode('utf-8-sig'))  # UTF-8 with BOM for Excel
        csv_bytes.seek(0)
        
        return csv_bytes
    
    def get_emotion_trends(self, user_id: int, days: int = 30) -> Dict:
        """Get emotion trends over time"""
        entries = self._get_entries_for_period(user_id, days)
        
        if not entries:
            return {}
        
        # Group by day
        daily_data = defaultdict(lambda: {'valence': [], 'arousal': [], 'count': 0})
        
        for entry in entries:
            day_key = entry.timestamp.strftime('%Y-%m-%d')
            daily_data[day_key]['valence'].append(entry.valence)
            daily_data[day_key]['arousal'].append(entry.arousal)
            daily_data[day_key]['count'] += 1
        
        # Calculate daily averages
        trends = {}
        for day, data in daily_data.items():
            trends[day] = {
                'avg_valence': sum(data['valence']) / len(data['valence']),
                'avg_arousal': sum(data['arousal']) / len(data['arousal']),
                'entry_count': data['count']
            }
        
        return trends
    
    def get_weekly_comparison(self, user_id: int) -> Dict:
        """Compare this week with previous week"""
        now = datetime.now(timezone.utc)
        
        # Current week (last 7 days)
        current_week_entries = self._get_entries_for_period(user_id, 7)
        
        # Previous week (8-14 days ago)
        prev_week_start = now - timedelta(days=14)
        prev_week_end = now - timedelta(days=7)
        
        with get_session() as session:
            prev_week_entries = session.query(Entry).filter(
                Entry.user_id == user_id,
                Entry.timestamp >= prev_week_start,
                Entry.timestamp < prev_week_end
            ).all()
        
        def analyze_week(entries):
            if not entries:
                return {'valence': 0, 'arousal': 0, 'count': 0, 'top_emotions': []}
            
            valence_avg = sum(e.valence for e in entries) / len(entries)
            arousal_avg = sum(e.arousal for e in entries) / len(entries)
            
            emotion_freq = self._get_emotion_frequency(entries)
            top_emotions = [emotion for emotion, count in emotion_freq.most_common(3)]
            
            return {
                'valence': valence_avg,
                'arousal': arousal_avg,
                'count': len(entries),
                'top_emotions': top_emotions
            }
        
        current_analysis = analyze_week(current_week_entries)
        previous_analysis = analyze_week(prev_week_entries)
        
        # Calculate changes
        valence_change = current_analysis['valence'] - previous_analysis['valence']
        arousal_change = current_analysis['arousal'] - previous_analysis['arousal']
        
        return {
            'current_week': current_analysis,
            'previous_week': previous_analysis,
            'changes': {
                'valence': valence_change,
                'arousal': arousal_change,
                'entries': current_analysis['count'] - previous_analysis['count']
            }
        }
    
    def get_emotion_correlations(self, user_id: int, days: int = 30) -> Dict:
        """Find correlations between emotions and times/triggers"""
        entries = self._get_entries_for_period(user_id, days)
        
        if not entries:
            return {}
        
        # Time-emotion correlations
        time_emotions = defaultdict(list)
        for entry in entries:
            hour = entry.timestamp.hour
            if entry.emotions:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                time_emotions[hour].extend(emotions)
        
        # Find most common emotions by time of day
        time_correlations = {}
        for hour, emotions in time_emotions.items():
            emotion_counts = Counter(emotions)
            if emotion_counts:
                most_common = emotion_counts.most_common(1)[0]
                time_correlations[hour] = {
                    'emotion': most_common[0],
                    'frequency': most_common[1],
                    'total_entries': len(emotions)
                }
        
        # Trigger-emotion patterns
        trigger_patterns = defaultdict(list)
        for entry in entries:
            if entry.cause and entry.emotions:
                cause_words = entry.cause.lower().split()
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                
                for word in cause_words:
                    if len(word) > 3:  # Filter short words
                        trigger_patterns[word].extend(emotions)
        
        # Find strongest trigger-emotion associations
        trigger_correlations = {}
        for trigger, emotions in trigger_patterns.items():
            if len(emotions) >= 2:  # At least 2 occurrences
                emotion_counts = Counter(emotions)
                most_common = emotion_counts.most_common(1)[0]
                trigger_correlations[trigger] = {
                    'emotion': most_common[0],
                    'frequency': most_common[1],
                    'total_entries': len(emotions)
                }
        
        return {
            'time_correlations': time_correlations,
            'trigger_correlations': dict(list(trigger_correlations.items())[:10])  # Top 10
        }
