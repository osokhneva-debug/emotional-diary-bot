# analysis.py
import json
import csv
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
from collections import Counter, defaultdict
import io
import logging

from db import get_session, User, Entry
from i18n import TEXTS, EMOTION_CATEGORIES

logger = logging.getLogger(__name__)

class EmotionAnalyzer:
    """Analyzes emotional data and generates insights"""
    
    def __init__(self):
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —ç–º–æ—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        self.emotion_groups = {
            'recovery_growth': {
                'name': 'üå± –≠–º–æ—Ü–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞',
                'keywords': [
                    # –†–∞–¥–æ—Å—Ç—å/–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ
                    '—Ä–∞–¥–æ—Å—Ç—å', '—Å—á–∞—Å—Ç—å–µ', '–≤–æ—Å—Ç–æ—Ä–≥', '–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å', '–≤–¥–æ—Ö–Ω–æ–≤–µ–Ω–∏–µ', '–≥–æ—Ä–¥–æ—Å—Ç—å', '—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '–±–ª–∞–∂–µ–Ω—Å—Ç–≤–æ',
                    # –ò–Ω—Ç–µ—Ä–µ—Å/–õ—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ  
                    '–∏–Ω—Ç–µ—Ä–µ—Å', '–ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ', '—É–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç—å', '–ø—Ä–µ–¥–≤–∫—É—à–µ–Ω–∏–µ', '–∞–∑–∞—Ä—Ç', '–∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å', '–ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                    # –°–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ/–£–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ
                    '—Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ', '—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–Ω–æ—Å—Ç—å', '–≥–∞—Ä–º–æ–Ω–∏—è', '–ø—Ä–∏–Ω—è—Ç–∏–µ', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '—É–º–∏—Ä–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ', '–±–µ–∑–º—è—Ç–µ–∂–Ω–æ—Å—Ç—å', '—Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ',
                    # –≠–Ω–µ—Ä–≥–∏—á–Ω–æ—Å—Ç—å/–í–æ–æ–¥—É—à–µ–≤–ª–µ–Ω–∏–µ
                    '–±–æ–¥—Ä–æ—Å—Ç—å', '—ç–Ω—Ç—É–∑–∏–∞–∑–º', '–≤–æ–∑–±—É–∂–¥–µ–Ω–∏–µ', '—Ä–µ—à–∏–º–æ—Å—Ç—å', '–¥—Ä–∞–π–≤', '–≤–æ–æ–¥—É—à–µ–≤–ª–µ–Ω–∏–µ', '–ø–æ–¥—ä—ë–º', '—ç–Ω–µ—Ä–≥–∏—è'
                ],
                'description': '–≠–º–æ—Ü–∏–∏, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—â–∏–µ —Ä–∞–∑–≤–∏—Ç–∏—é –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é'
            },
            'tension_signal': {
                'name': 'üå™ –≠–º–æ—Ü–∏–∏ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Å–∏–≥–Ω–∞–ª–∞',
                'keywords': [
                    # –¢—Ä–µ–≤–æ–≥–∞/–ë–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ
                    '—Ç—Ä–µ–≤–æ–≥–∞', '–±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ', '–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç—å', '—Å—Ç—Ä–∞—Ö', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ', '—Å–æ–º–Ω–µ–Ω–∏–µ', '–≤–æ–ª–Ω–µ–Ω–∏–µ', '–æ–ø–∞—Å–µ–Ω–∏–µ',
                    # –ì—Ä—É—Å—Ç—å/–ü–µ—á–∞–ª—å
                    '–≥—Ä—É—Å—Ç—å', '–ø–µ—á–∞–ª—å', '—Ç–æ—Å–∫–∞', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ', '–º–µ–ª–∞–Ω—Ö–æ–ª–∏—è', '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ', '–≥–æ—Ä–µ', '—É–Ω—ã–Ω–∏–µ', '—Å–æ–∂–∞–ª–µ–Ω–∏–µ',
                    # –ó–ª–æ—Å—Ç—å/–†–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ
                    '–∑–ª–æ—Å—Ç—å', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ', '–≥–Ω–µ–≤', '–æ–±–∏–¥–∞', '—Ñ—Ä—É—Å—Ç—Ä–∞—Ü–∏—è', '–∑–∞–≤–∏—Å—Ç—å', '–≤–æ–∑–º—É—â–µ–Ω–∏–µ', '–Ω–µ–≥–æ–¥–æ–≤–∞–Ω–∏–µ',
                    # –°—Ç—ã–¥/–í–∏–Ω–∞
                    '—Å—Ç—ã–¥', '–≤–∏–Ω–∞', '—Å–º—É—â–µ–Ω–∏–µ', '–Ω–µ–ª–æ–≤–∫–æ—Å—Ç—å', '—Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞', '—É–≥—Ä—ã–∑–µ–Ω–∏—è', '—Ä–∞—Å–∫–∞—è–Ω–∏–µ',
                    # –£—Å—Ç–∞–ª–æ—Å—Ç—å/–ò—Å—Ç–æ—â–µ–Ω–∏–µ
                    '—É—Å—Ç–∞–ª–æ—Å—Ç—å', '–∏—Å—Ç–æ—â–µ–Ω–∏–µ', '–∞–ø–∞—Ç–∏—è', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', '–±–µ–∑—Ä–∞–∑–ª–∏—á–∏–µ', '–≤—è–ª–æ—Å—Ç—å', '–∏–∑–Ω–µ–º–æ–∂–µ–Ω–∏–µ', '–æ–ø—É—Å—Ç–æ—à—ë–Ω–Ω–æ—Å—Ç—å',
                    # –£–¥–∏–≤–ª–µ–Ω–∏–µ/–®–æ–∫
                    '—É–¥–∏–≤–ª–µ–Ω–∏–µ', '–∏–∑—É–º–ª–µ–Ω–∏–µ', '–æ—à–µ–ª–æ–º–ª–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–µ—Ä—è–Ω–Ω–æ—Å—Ç—å', '–Ω–µ–¥–æ—É–º–µ–Ω–∏–µ', '–ø–æ—Ç—Ä—è—Å–µ–Ω–∏–µ', '—à–æ–∫'
                ],
                'description': '–≠–º–æ—Ü–∏–∏, —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–µ –æ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è—Ö –∏ –≤—ã–∑–æ–≤–∞—Ö'
            },
            'neutral': {
                'name': '‚öñ –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ/–ø—Ä–æ—á–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è',
                'keywords': [
                    '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ', '–æ–±—ã—á–Ω–æ', '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–µ—è—Å–Ω–æ—Å—Ç—å', '—Å–º–µ—à–∞–Ω–Ω–æ—Å—Ç—å'
                ],
                'description': '–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è'
            }
        }
    
    async def generate_summary(self, user_id: int, period: str) -> str:
        """Generate emotional summary for specified period"""
        try:
            # Parse period
            days = self._parse_period(period)
            if not days:
                return TEXTS.get('invalid_period', '–ù–µ–≤–µ—Ä–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.')
            
            # Get entries for the period
            entries = self._get_entries_for_period(user_id, days)
            
            if not entries:
                period_name = {7: "–Ω–µ–¥–µ–ª—é", 14: "2 –Ω–µ–¥–µ–ª–∏", 30: "–º–µ—Å—è—Ü", 90: "3 –º–µ—Å—è—Ü–∞"}.get(days, f"{days} –¥–Ω–µ–π")
                return f"üì≠ –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π –Ω–µ—Ç –∑–∞–ø–∏—Å–µ–π —ç–º–æ—Ü–∏–π.\n\n–ù–∞—á–Ω–∏—Ç–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ —ç–º–æ—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é /note!"
            
            # Generate comprehensive analysis
            summary_parts = []
            
            # Header
            period_name = {7: "–Ω–µ–¥–µ–ª—é", 14: "2 –Ω–µ–¥–µ–ª–∏", 30: "–º–µ—Å—è—Ü", 90: "3 –º–µ—Å—è—Ü–∞"}.get(days, f"{days} –¥–Ω–µ–π")
            summary_parts.append(f"üìä <b>–°–≤–æ–¥–∫–∞ –∑–∞ {period_name}</b>")
            summary_parts.append("")
            
            # Emotion groups analysis
            group_analysis = self._analyze_emotion_groups(entries)
            emotion_details = self._get_emotion_details_by_group(entries)
            
            summary_parts.append("<b>üé≠ –≠–º–æ—Ü–∏–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º:</b>")
            summary_parts.append("")
            
            for group_key, group_data in self.emotion_groups.items():
                count = group_analysis.get(group_key, 0)
                group_name = group_data['name']
                
                summary_parts.append(f"<b>{group_name}:</b> {count} —Ä–∞–∑")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ —ç–º–æ—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
                if count > 0 and group_key in emotion_details:
                    emotions_list = []
                    for emotion, freq in emotion_details[group_key].most_common(5):
                        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —ç–º–æ—Ü–∏–π
                        emotion_escaped = self._escape_html(emotion)
                        emotions_list.append(f'"{emotion_escaped}" ({freq})')
                    if emotions_list:
                        summary_parts.append(", ".join(emotions_list))
                
                summary_parts.append("")
            
            # Trigger analysis by groups
            trigger_analysis = self._analyze_triggers_by_groups(entries)
            if trigger_analysis:
                summary_parts.append("<b>üîç –ß—Ç–æ –≤–ª–∏—è–ª–æ –Ω–∞ —ç–º–æ—Ü–∏–∏:</b>")
                summary_parts.append("")
                
                for group_key, triggers in trigger_analysis.items():
                    if triggers:
                        group_name = self.emotion_groups[group_key]['name']
                        # –°–æ–∫—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
                        if '–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞' in group_name:
                            trigger_title = "üå± –¢—Ä–∏–≥–≥–µ—Ä—ã –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —ç–º–æ—Ü–∏–π:"
                        elif '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Å–∏–≥–Ω–∞–ª–∞' in group_name:
                            trigger_title = "üå™ –¢—Ä–∏–≥–≥–µ—Ä—ã –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π:"
                        else:
                            trigger_title = "‚öñ –¢—Ä–∏–≥–≥–µ—Ä—ã –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö —ç–º–æ—Ü–∏–π:"
                        
                        summary_parts.append(f"<b>{trigger_title}</b>")
                        for trigger in triggers[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-3
                            trigger_escaped = self._escape_html(trigger)
                            summary_parts.append(f"‚Ä¢ {trigger_escaped}")
                        summary_parts.append("")
            
            # Time patterns
            time_patterns = self._analyze_time_patterns(entries)
            if time_patterns:
                peak_hour = max(time_patterns.items(), key=lambda x: x[1])
                time_of_day = self._get_time_of_day_name(peak_hour[0])
                summary_parts.append(f"<b>‚è∞ –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:</b> {peak_hour[0]}:00 ({time_of_day})")
            
            summary_parts.append(f"<b>üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:</b> {len(entries)}")
            summary_parts.append("")
            
            # Add insights for working women
            insights = self._generate_insights_for_working_women(entries, group_analysis)
            if insights:
                summary_parts.append("<b>üí° –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã:</b>")
                for insight in insights:
                    summary_parts.append(f"‚Ä¢ {insight}")
                summary_parts.append("")
            
            # Footer
            summary_parts.append("<i>–•–æ—á–µ—à—å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏? –ò—Å–ø–æ–ª—å–∑—É–π /export –¥–ª—è CSV-—Ñ–∞–π–ª–∞.</i>")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"Error generating summary for user {user_id}: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _escape_html(self, text: str) -> str:
        """Escape HTML characters in text"""
        if not text:
            return ""
        return text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    
    def _analyze_emotion_groups(self, entries: List[Entry]) -> Dict[str, int]:
        """Analyze emotion distribution by groups using keyword matching"""
        group_counts = defaultdict(int)
        
        for entry in entries:
            if not entry.emotions:
                continue
                
            try:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
            except (json.JSONDecodeError, TypeError):
                continue
            
            for emotion in emotions:
                emotion_lower = emotion.lower().strip()
                group_found = False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É
                for group_key, group_data in self.emotion_groups.items():
                    keywords = group_data['keywords']
                    
                    # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    for keyword in keywords:
                        if (emotion_lower == keyword.lower() or 
                            keyword.lower() in emotion_lower or 
                            emotion_lower in keyword.lower()):
                            group_counts[group_key] += 1
                            group_found = True
                            break
                    
                    if group_found:
                        break
                
                # –ï—Å–ª–∏ —ç–º–æ—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –≤ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø–µ, –æ—Ç–Ω–æ—Å–∏–º –∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º
                if not group_found:
                    group_counts['neutral'] += 1
        
        return dict(group_counts)
    
    def _get_emotion_details_by_group(self, entries: List[Entry]) -> Dict[str, Counter]:
        """Get detailed emotion counts for each group"""
        group_emotions = defaultdict(Counter)
        
        for entry in entries:
            if not entry.emotions:
                continue
                
            try:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
            except (json.JSONDecodeError, TypeError):
                continue
            
            for emotion in emotions:
                emotion_lower = emotion.lower().strip()
                group_found = False
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É
                for group_key, group_data in self.emotion_groups.items():
                    keywords = group_data['keywords']
                    
                    # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    for keyword in keywords:
                        if (emotion_lower == keyword.lower() or 
                            keyword.lower() in emotion_lower or 
                            emotion_lower in keyword.lower()):
                            group_emotions[group_key][emotion] += 1
                            group_found = True
                            break
                    
                    if group_found:
                        break
                
                # –ï—Å–ª–∏ —ç–º–æ—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –≤ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø–µ, –æ—Ç–Ω–æ—Å–∏–º –∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º
                if not group_found:
                    group_emotions['neutral'][emotion] += 1
        
        return dict(group_emotions)
    
    def _analyze_triggers_by_groups(self, entries: List[Entry]) -> Dict[str, List[str]]:
        """Analyze triggers grouped by emotion groups"""
        group_triggers = defaultdict(Counter)
        
        for entry in entries:
            if not entry.cause or not entry.emotions:
                continue
                
            try:
                emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
            except (json.JSONDecodeError, TypeError):
                continue
                
            cause = entry.cause.strip()
            
            if len(cause) < 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏—á–∏–Ω—ã
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π –≥—Ä—É–ø–ø–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —ç—Ç–∞ –∑–∞–ø–∏—Å—å
            emotion_group = None
            for emotion in emotions:
                emotion_lower = emotion.lower().strip()
                
                for group_key, group_data in self.emotion_groups.items():
                    keywords = group_data['keywords']
                    
                    for keyword in keywords:
                        if (emotion_lower == keyword.lower() or 
                            keyword.lower() in emotion_lower or 
                            emotion_lower in keyword.lower()):
                            emotion_group = group_key
                            break
                    
                    if emotion_group:
                        break
                
                if emotion_group:
                    break
            
            # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –æ—Ç–Ω–æ—Å–∏–º –∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–º
            if not emotion_group:
                emotion_group = 'neutral'
            
            group_triggers[emotion_group][cause] += 1
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–∫–∏ —Ç–æ–ø —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        result = {}
        for group, triggers_counter in group_triggers.items():
            result[group] = [trigger for trigger, _ in triggers_counter.most_common(5)]
        
        return result
    
    def _generate_insights_for_working_women(self, entries: List[Entry], group_analysis: Dict[str, int]) -> List[str]:
        """Generate personalized insights for working women"""
        insights = []
        
        total_entries = len(entries)
        if total_entries == 0:
            return insights
        
        # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞ —ç–º–æ—Ü–∏–π
        positive_count = group_analysis.get('recovery_growth', 0)
        negative_count = group_analysis.get('tension_signal', 0)
        
        positive_ratio = positive_count / total_entries if total_entries > 0 else 0
        negative_ratio = negative_count / total_entries if total_entries > 0 else 0
        
        # –ò–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–ª–∞–Ω—Å–∞ —ç–º–æ—Ü–∏–π
        if negative_ratio > 0.7:
            insights.append("–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Å—Ç—Ä–µ—Å—Å–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞.")
        elif positive_ratio > 0.6:
            insights.append("–û—Ç–ª–∏—á–Ω—ã–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å! –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –∑–∞–±–æ—Ç–∏—Ç—å—Å—è –æ —Å–µ–±–µ.")
        elif negative_ratio > positive_ratio:
            insights.append("–ë–æ–ª—å—à–µ —Å–∏–≥–Ω–∞–ª—å–Ω—ã—Ö —ç–º–æ—Ü–∏–π. –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É.")
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        time_patterns = self._analyze_time_patterns(entries)
        if time_patterns:
            morning_entries = sum(count for hour, count in time_patterns.items() if 6 <= hour <= 12)
            evening_entries = sum(count for hour, count in time_patterns.items() if 18 <= hour <= 23)
            
            if evening_entries > morning_entries * 1.5:
                insights.append("–ë–æ–ª—å—à–µ –∑–∞–ø–∏—Å–µ–π –≤–µ—á–µ—Ä–æ–º. –°–æ–∑–¥–∞–π—Ç–µ —Ä–∏—Ç—É–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è.")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —ç–º–æ—Ü–∏–π
        unique_emotions = set()
        for entry in entries:
            if entry.emotions:
                try:
                    emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                    unique_emotions.update(emotions)
                except (json.JSONDecodeError, TypeError):
                    continue
        
        emotion_variety = len(unique_emotions)
        if emotion_variety < 5 and total_entries > 10:
            insights.append("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–∞–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è.")
        elif emotion_variety > 15:
            insights.append("–ë–æ–≥–∞—Ç—ã–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –ø–æ–º–æ–≥–∞–µ—Ç –≤–∞–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å —Å–µ–±—è!")
        
        return insights[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–Ω—Å–∞–π—Ç–∞
    
    def _get_time_of_day_name(self, hour: int) -> str:
        """Get human-readable time of day name"""
        if 6 <= hour < 12:
            return "—É—Ç—Ä–æ–º"
        elif 12 <= hour < 17:
            return "–¥–Ω—ë–º"
        elif 17 <= hour < 22:
            return "–≤–µ—á–µ—Ä–æ–º"
        else:
            return "–Ω–æ—á—å—é"
    
    def _parse_period(self, period: str) -> Optional[int]:
        """Parse period string to number of days"""
        period_map = {
            "7": 7,
            "14": 14,
            "30": 30,
            "90": 90
        }
        return period_map.get(period)
    
    def _get_entries_for_period(self, user_id: int, days: int) -> List[Entry]:
        """Get entries for the specified period"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
        
        try:
            with get_session() as session:
                return session.query(Entry).filter(
                    Entry.user_id == user_id,
                    Entry.timestamp >= cutoff_date
                ).order_by(Entry.timestamp.desc()).all()
        except Exception as e:
            logger.error(f"Error getting entries for user {user_id}: {e}")
            return []
    
    def _analyze_time_patterns(self, entries: List[Entry]) -> Dict[int, int]:
        """Analyze emotional activity by hour of day"""
        hour_counts = defaultdict(int)
        
        for entry in entries:
            if entry.timestamp:
                hour = entry.timestamp.hour
                hour_counts[hour] += 1
        
        return dict(hour_counts)
    
    def generate_csv_export(self, entries: List[Entry]) -> io.BytesIO:
        """Generate CSV export of emotional data"""
        output = io.StringIO()
        
        fieldnames = [
            '–î–∞—Ç–∞', '–í—Ä–µ–º—è', '–≠–º–æ—Ü–∏–∏', '–ì—Ä—É–ø–ø–∞_—ç–º–æ—Ü–∏–π', '–ü—Ä–∏—á–∏–Ω–∞', '–ó–∞–º–µ—Ç–∫–∏', '–í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å', '–ê–∫—Ç–∏–≤–∞—Ü–∏—è'
        ]
        
        writer = csv.DictWriter(output, fieldnames=fieldnames)
        writer.writeheader()
        
        for entry in entries:
            try:
                emotions_str = ''
                emotion_group = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ"
                
                if entry.emotions:
                    emotions = json.loads(entry.emotions) if isinstance(entry.emotions, str) else entry.emotions
                    emotions_str = ', '.join(emotions)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É —ç–º–æ—Ü–∏–π –¥–ª—è CSV
                    for emotion in emotions:
                        emotion_lower = emotion.lower().strip()
                        
                        for group_key, group_data in self.emotion_groups.items():
                            keywords = group_data['keywords']
                            
                            for keyword in keywords:
                                if (emotion_lower == keyword.lower() or 
                                    keyword.lower() in emotion_lower or 
                                    emotion_lower in keyword.lower()):
                                    if group_key == 'recovery_growth':
                                        emotion_group = "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ —Ä–æ—Å—Ç–∞"
                                    elif group_key == 'tension_signal':
                                        emotion_group = "–ù–∞–ø—Ä—è–∂–µ–Ω–∏—è –∏ —Å–∏–≥–Ω–∞–ª–∞"
                                    else:
                                        emotion_group = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ"
                                    break
                            
                            if emotion_group != "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ":
                                break
                        
                        if emotion_group != "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ":
                            break
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏—é –≤ —Ç–µ–∫—Å—Ç
                valence_text = ""
                if entry.valence is not None:
                    if entry.valence > 0.3:
                        valence_text = "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è"
                    elif entry.valence < -0.3:
                        valence_text = "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è"
                    else:
                        valence_text = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è"
                
                arousal_text = ""
                if entry.arousal is not None:
                    if entry.arousal > 1.3:
                        arousal_text = "–í—ã—Å–æ–∫–∞—è"
                    elif entry.arousal < 0.7:
                        arousal_text = "–ù–∏–∑–∫–∞—è"
                    else:
                        arousal_text = "–°—Ä–µ–¥–Ω—è—è"
                
                writer.writerow({
                    '–î–∞—Ç–∞': entry.timestamp.strftime('%Y-%m-%d') if entry.timestamp else '',
                    '–í—Ä–µ–º—è': entry.timestamp.strftime('%H:%M:%S') if entry.timestamp else '',
                    '–≠–º–æ—Ü–∏–∏': emotions_str,
                    '–ì—Ä—É–ø–ø–∞_—ç–º–æ—Ü–∏–π': emotion_group,
                    '–ü—Ä–∏—á–∏–Ω–∞': entry.cause or '',
                    '–ó–∞–º–µ—Ç–∫–∏': entry.notes or '',
                    '–í–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç—å': valence_text,
                    '–ê–∫—Ç–∏–≤–∞—Ü–∏—è': arousal_text
                })
            except Exception as e:
                logger.error(f"Error processing entry {entry.id}: {e}")
                continue
        
        # Convert to bytes
        output.seek(0)
        csv_bytes = io.BytesIO()
        csv_bytes.write(output.getvalue().encode('utf-8-sig'))  # UTF-8 with BOM for Excel
        csv_bytes.seek(0)
        
        return csv_bytes
